{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# NMT-Keras tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creating and training a Neural Translation Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll create and train a Neural Machine Translation (NMT) model. Since there is a significant number of hyperparameters, we'll use the default ones, specified in the `config.py` file. Note that almost every hardcoded parameter is automatically set from config if we run  `main.py `.\n",
    "\n",
    "We'll create the so-called `'GroundHogModel'`. It is defined in the `model_zoo.py` file. See the `neural_machine_translation.pdf` for an overview of such system.\n",
    "\n",
    "If you followed the notebook `1_dataset_tutorial.ipynb`, you should have a dataset instance. Otherwise, you should follow that notebook first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll make some imports, load the default parameters and load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30/11/2016_16:33:14:  <<< Loading Dataset instance from datasets/Dataset_tutorial_dataset.pkl ... >>>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30/11/2016_16:33:14:  <<< Dataset instance loaded >>>\n"
     ]
    }
   ],
   "source": [
    "from config import load_parameters\n",
    "from model_zoo import TranslationModel\n",
    "import utils\n",
    "from keras_wrapper.cnn_model import loadModel\n",
    "from keras_wrapper.dataset import loadDataset\n",
    "params = load_parameters()\n",
    "dataset = loadDataset('datasets/Dataset_tutorial_dataset.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the number of words in the dataset may be unknown beforehand, we must update the params information according to the dataset instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['INPUT_VOCABULARY_SIZE'] = dataset.vocabulary_len['source_text']\n",
    "params['OUTPUT_VOCABULARY_SIZE'] = dataset.vocabulary_len['target_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we create a TranslationModel instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30/11/2016_16:33:18:  <<< Building GroundHogModel Translation_Model >>>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------\n\t\tTranslationModel instance\n-----------------------------------------------------------------------------------\n_model_type: GroundHogModel\nname: tutorial_model\nmodel_path: trained_models/tutorial_model/\nverbose: True\n\nMODEL PARAMETERS:\n{'SOURCE_GLOVE_VECTORS': None, 'SAMPLE_ON_SETS': ['train', 'val'], 'CLIP_C': 1.0, 'HEURISTIC': 1, 'EVAL_EACH': 1, 'TRG_LAN': 'en', 'SAMPLING': 'max_likelihood', 'SAMPLE_EACH_UPDATES': 2500, 'N_LAYERS_DECODER': 1, 'POS_UNK': False, 'ENCODER_HIDDEN_SIZE': 600, 'REBUILD_DATASET': True, 'METRICS': ['coco'], 'OPTIMIZER': 'Adam', 'SOURCE_TEXT_EMBEDDING_SIZE': 420, 'EVAL_EACH_EPOCHS': True, 'EPOCHS_FOR_SAVE': 1, 'USE_BATCH_NORMALIZATION': True, 'BATCH_SIZE': 50, 'MODEL_NAME': 'ue_GroundHogModel_src_emb_420_bidir_True_enc_600_dec_600_deepout_maxout_trg_emb_420_Adam_0.001', 'BATCH_NORMALIZATION_MODE': 1, 'N_SAMPLES': 5, 'PARALLEL_LOADERS': 8, 'MIN_OCCURRENCES_VOCAB': 0, 'OUTPUTS_IDS_DATASET': ['target_text'], 'INIT_LAYERS': ['tanh'], 'EARLY_STOP': True, 'DATA_AUGMENTATION': False, 'TEXT_FILES': {'test': 'DATA/test.', 'train': 'DATA/training.', 'val': 'DATA/dev.'}, 'START_SAMPLING_ON_EPOCH': 1, 'SAMPLE_WEIGHTS': True, 'SAMPLING_SAVE_MODE': 'list', 'EXTRA_NAME': '', 'LOSS': 'categorical_crossentropy', 'WRITE_VALID_SAMPLES': True, 'INPUTS_IDS_MODEL': ['source_text', 'state_below'], 'MODE': 'training', 'LR_GAMMA': 0.8, 'NOISE_AMOUNT': 0.01, 'STOP_METRIC': 'Bleu_4', 'N_LAYERS_ENCODER': 1, 'WEIGHT_DECAY': 0.0001, 'BIDIRECTIONAL_ENCODER': True, 'MAX_OUTPUT_TEXT_LEN_TEST': 120, 'FORCE_RELOAD_VOCABULARY': False, 'layer': ('maxout', 210), 'TOKENIZATION_METHOD': 'tokenize_none', 'DEEP_OUTPUT_LAYERS': [('maxout', 210)], 'OUTPUT_VOCABULARY_SIZE': 516, 'START_EVAL_ON_EPOCH': 10, 'BEAM_SEARCH': True, 'TARGET_TEXT_EMBEDDING_SIZE': 420, 'DECODER_HIDDEN_SIZE': 600, 'MODEL_TYPE': 'GroundHogModel', 'MAX_INPUT_TEXT_LEN': 50, 'EVAL_ON_SETS_KERAS': [], 'NORMALIZE_SAMPLING': True, 'MAX_OUTPUT_TEXT_LEN': 50, 'PAD_ON_BATCH': True, 'USE_PRELU': False, 'INPUT_VOCABULARY_SIZE': 689, 'BEAM_SIZE': 12, 'TRAIN_ON_TRAINVAL': False, 'LR': 0.001, 'SRC_LAN': 'es', 'CLASSIFIER_ACTIVATION': 'softmax', 'FILL': 'end', 'ALPHA_FACTOR': 0.6, 'TEMPERATURE': 1, 'STORE_PATH': 'trained_models/ue_GroundHogModel_src_emb_420_bidir_True_enc_600_dec_600_deepout_maxout_trg_emb_420_Adam_0.001/', 'DATASET_STORE_PATH': 'datasets/', 'DROPOUT_P': 0.5, 'DATA_ROOT_PATH': '/media/HDD_2TB/DATASETS/ue/', 'HOMOGENEOUS_BATCHES': False, 'LR_DECAY': 20, 'DATASET_NAME': 'ue', 'USE_DROPOUT': False, 'INPUTS_IDS_DATASET': ['source_text', 'state_below'], 'VERBOSE': 1, 'PATIENCE': 20, 'OUTPUTS_IDS_MODEL': ['target_text'], 'USE_NOISE': True, 'TARGET_GLOVE_VECTORS': None, 'TARGET_GLOVE_VECTORS_TRAINABLE': True, 'RELOAD': 0, 'EVAL_ON_SETS': ['val'], 'MAX_EPOCH': 500, 'SOURCE_GLOVE_VECTORS_TRAINABLE': True, 'USE_L2': False}\n-----------------------------------------------------------------------------------\n____________________________________________________________________________________________________\nLayer (type)                     Output Shape          Param #     Connected to                     \n====================================================================================================\nsource_text (InputLayer)         (None, None)          0                                            \n____________________________________________________________________________________________________\nsource_word_embedding (Embedding)(None, None, 420)     289380      source_text[0][0]                \n____________________________________________________________________________________________________\nsrc_embedding_gaussian_noise (Gau(None, None, 420)     0           source_word_embedding[0][0]      \n____________________________________________________________________________________________________\nsrc_embedding_batch_normalization(None, None, 420)     840         src_embedding_gaussian_noise[0][0\n____________________________________________________________________________________________________\nbidirectional_encoder (Bidirectio(None, None, 1200)    3675600     src_embedding_batch_normalization\n____________________________________________________________________________________________________\nannotations_gaussian_noise (Gauss(None, None, 1200)    0           bidirectional_encoder[0][0]      \n____________________________________________________________________________________________________\nannotations_batch_normalization ((None, None, 1200)    2400        annotations_gaussian_noise[0][0] \n____________________________________________________________________________________________________\nstate_below (InputLayer)         (None, None)          0                                            \n____________________________________________________________________________________________________\nmaskedmean_2 (MaskedMean)        (None, 1200)          0           annotations_batch_normalization[0\n____________________________________________________________________________________________________\ntarget_word_embedding (Embedding)(None, None, 420)     216720      state_below[0][0]                \n____________________________________________________________________________________________________\ninitial_state (Dense)            (None, 600)           720600      maskedmean_2[0][0]               \n____________________________________________________________________________________________________\nstate_below_gaussian_noise (Gauss(None, None, 420)     0           target_word_embedding[0][0]      \n____________________________________________________________________________________________________\ninitial_state_gaussian_noise (Gau(None, 600)           0           initial_state[0][0]              \n____________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state_below_batch_normalization ((None, None, 420)     840         state_below_gaussian_noise[0][0] \n____________________________________________________________________________________________________\ninitial_state_batch_normalization(None, 600)           1200        initial_state_gaussian_noise[0][0\n____________________________________________________________________________________________________\nattgrucond_2 (AttGRUCond)        [(None, None, 600), (N6160201     state_below_batch_normalization[0\n                                                                   annotations_batch_normalization[0\n                                                                   initial_state_batch_normalization\n____________________________________________________________________________________________________\nproj_h0_gaussian_noise (GaussianN(None, None, 600)     0           attgrucond_2[0][0]               \n____________________________________________________________________________________________________\nproj_h0_batch_normalization (Batc(None, None, 600)     1200        proj_h0_gaussian_noise[0][0]     \n____________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logit_ctx (TimeDistributed)      multiple              504420      attgrucond_2[0][1]               \n____________________________________________________________________________________________________\nlogit_lstm (TimeDistributed)     multiple              252420      proj_h0_batch_normalization[0][0]\n____________________________________________________________________________________________________\nlambda_2 (Lambda)                (None, None, 420)     0           logit_ctx[0][0]                  \n____________________________________________________________________________________________________\nlogit_emb (TimeDistributed)      multiple              176820      state_below_batch_normalization[0\n____________________________________________________________________________________________________\nout_layer_mlp_gaussian_noise (Gau(None, None, 420)     0           logit_lstm[0][0]                 \n____________________________________________________________________________________________________\nout_layer_ctx_gaussian_noise (Gau(None, None, 420)     0           lambda_2[0][0]                   \n____________________________________________________________________________________________________\nout_layer_emb_gaussian_noise (Gau(None, None, 420)     0           logit_emb[0][0]                  \n____________________________________________________________________________________________________\nout_layer_mlp_batch_normalization(None, None, 420)     840         out_layer_mlp_gaussian_noise[0][0\n____________________________________________________________________________________________________\nout_layer_ctx_batch_normalization(None, None, 420)     840         out_layer_ctx_gaussian_noise[0][0\n____________________________________________________________________________________________________\nout_layer_emb_batch_normalization(None, None, 420)     840         out_layer_emb_gaussian_noise[0][0\n____________________________________________________________________________________________________\nadditional_input (Merge)         (None, None, 420)     0           out_layer_mlp_batch_normalization\n                                                                   out_layer_ctx_batch_normalization\n                                                                   out_layer_emb_batch_normalization\n____________________________________________________________________________________________________\nactivation_2 (Activation)        (None, None, 420)     0           additional_input[0][0]           \n____________________________________________________________________________________________________\nmaxout_0 (TimeDistributed)       multiple              353640      activation_2[0][0]               \n____________________________________________________________________________________________________\nout_layermaxout_gaussian_noise (G(None, None, 210)     0           maxout_0[0][0]                   \n____________________________________________________________________________________________________\nout_layermaxout_batch_normalizati(None, None, 210)     420         out_layermaxout_gaussian_noise[0]\n____________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_text (TimeDistributed)    multiple              108876      out_layermaxout_batch_normalizati\n====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30/11/2016_16:33:21:  Preparing optimizer and compiling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 12468097\n____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "nmt_model = TranslationModel(params,\n",
    "                             type='GroundHogModel', \n",
    "                             model_name='tutorial_model',\n",
    "                             vocabularies=dataset.vocabulary,\n",
    "                             store_path='trained_models/tutorial_model/',\n",
    "                             verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we must define the inputs and outputs mapping from our Dataset instance to our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputMapping = dict()\n",
    "for i, id_in in enumerate(params['INPUTS_IDS_DATASET']):\n",
    "    pos_source = dataset.ids_inputs.index(id_in)\n",
    "    id_dest = nmt_model.ids_inputs[i]\n",
    "    inputMapping[id_dest] = pos_source\n",
    "nmt_model.setInputsMapping(inputMapping)\n",
    "\n",
    "outputMapping = dict()\n",
    "for i, id_out in enumerate(params['OUTPUTS_IDS_DATASET']):\n",
    "    pos_target = dataset.ids_outputs.index(id_out)\n",
    "    id_dest = nmt_model.ids_outputs[i]\n",
    "    outputMapping[id_dest] = pos_target\n",
    "nmt_model.setOutputsMapping(outputMapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add some callbacks for controlling the training (e.g. Sampling each N updates, early stop, learning rate annealing...). For instance, let's build an Early-Stop callback. After each 2 epochs, it will compute the 'coco' scores on the development set. If the metric 'Bleu_4' doesn't improve during more than 5 checkings, it will stop. We need to pass some variables to the callback (in the extra_vars dictionary):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_vars = {'language': 'en', \n",
    "              'n_parallel_loaders': 8,\n",
    "              'tokenize_f': eval('dataset.' + 'tokenize_none'),\n",
    "              'beam_size': 12,\n",
    "              'maxlen': 50,\n",
    "              'model_inputs': ['source_text', 'state_below'],\n",
    "              'model_outputs': ['target_text'],\n",
    "              'dataset_inputs': ['source_text', 'state_below'],\n",
    "              'dataset_outputs': ['target_text'],\n",
    "              'normalize': True,\n",
    "              'alpha_factor': 0.6,\n",
    "              'val':{'references': dataset.extra_variables['val']['target_text']}\n",
    "              }\n",
    "\n",
    "vocab = dataset.vocabulary['target_text']['idx2words']\n",
    "callbacks = []\n",
    "callbacks.append(utils.callbacks.PrintPerformanceMetricOnEpochEnd(nmt_model,\n",
    "                                                 dataset,\n",
    "                                                 gt_id='target_text',\n",
    "                                                 metric_name=['coco'],\n",
    "                                                 set_name=['val'],\n",
    "                                                 batch_size=50,\n",
    "                                                 each_n_epochs=2,\n",
    "                                                 extra_vars=extra_vars,\n",
    "                                                 reload_epoch=0,\n",
    "                                                 is_text=True,\n",
    "                                                 index2word_y=vocab,\n",
    "                                                 sampling_type='max_likelihood',\n",
    "                                                 beam_search=True,\n",
    "                                                 save_path=nmt_model.model_path,\n",
    "                                                 start_eval_on_epoch=0,\n",
    "                                                 write_samples=True,\n",
    "                                                 write_type='list',\n",
    "                                                 early_stop=True,\n",
    "                                                 patience=5,\n",
    "                                                 stop_metric='Bleu_4',\n",
    "                                                 verbose=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are almost ready to train. We set up some training parameters..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_params = {'n_epochs': 100,\n",
    "                   'batch_size': 40,\n",
    "                   'maxlen': 30,\n",
    "                   'epochs_for_save': 1,\n",
    "                   'verbose': 0,\n",
    "                   'eval_on_sets': [], \n",
    "                   'n_parallel_loaders': 8,\n",
    "                   'extra_callbacks': callbacks,\n",
    "                   'reload_epoch': 0,\n",
    "                   'epoch_offset': 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmt_model.trainNet(dataset, training_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}