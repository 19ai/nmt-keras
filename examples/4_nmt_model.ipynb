{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# NMT-Keras tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this module, we are going to create an encoder-decoder model with:\n",
    "* A bidirectional GRU encoder and a GRU decoder\n",
    "* An attention model \n",
    "* The previously generated word feeds back de decoder\n",
    "* MLPs for initializing the initial RNN state\n",
    "* Skip connections from inputs to outputs\n",
    "* Beam search.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, first we import the necessary stuff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.engine import Input\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import GRU, AttGRUCond\n",
    "from keras.layers import TimeDistributed, Bidirectional\n",
    "from keras.layers.core import Dense, Activation, Lambda, MaxoutDense, MaskedMean\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import merge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And define the dimesnions of our model. For instance, a word embedding size of 50 and 100 units in RNNs. The inputs/outpus are defined as in previous tutorials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_inputs = ['source_text', 'state_below']\n",
    "ids_outputs = ['target_text']\n",
    "word_embedding_size = 50\n",
    "hidden_state_size = 100\n",
    "input_vocabulary_size=686  # Autoset in the library\n",
    "output_vocabulary_size=513  # Autoset in the library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's define our encoder. First, we have to create an Input layer to connect the input text to our model.  Next, we'll apply a word embedding to the sequence of input indices. This word embedding will feed a Bidirectional GRU network, which will produce our sequence of annotations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Source text input\n",
    "src_text = Input(name=ids_inputs[0],\n",
    "                 batch_shape=tuple([None, None]), # Since the input sequences have variable-length, we do not retrict the Input shape\n",
    "                 dtype='int32')\n",
    "# 2. Encoder\n",
    "# 2.1. Source word embedding\n",
    "src_embedding = Embedding(input_vocabulary_size, word_embedding_size, \n",
    "                          name='source_word_embedding', mask_zero=True # Zeroes as mask\n",
    "                          )(src_text)\n",
    "# 2.2. BRNN encoder (GRU/LSTM)\n",
    "annotations = Bidirectional(GRU(hidden_state_size, \n",
    "                                return_sequences=True  # Return the full sequence\n",
    "                                ),\n",
    "                            name='bidirectional_encoder',\n",
    "                            merge_mode='concat')(src_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have built the encoder, let's build our decoder.  First, we have an additional input: The previously generated word (the so-called state_below). We introduce it by means of an Input layer and a (target language) word embedding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Decoder\n",
    "# 3.1.1. Previously generated words as inputs for training -> Teacher forcing\n",
    "next_words = Input(name=ids_inputs[1], batch_shape=tuple([None, None]), dtype='int32')\n",
    "# 3.1.2. Target word embedding\n",
    "state_below = Embedding(output_vocabulary_size, word_embedding_size,\n",
    "                        name='target_word_embedding', \n",
    "                        mask_zero=True)(next_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial hidden state of the decoder's GRU is initialized by means of a MLP (in this case, single-layered) from the average of the annotations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx_mean = MaskedMean()(annotations)\n",
    "initial_state = Dense(hidden_state_size, name='initial_state',\n",
    "                      activation='tanh')(ctx_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we have the input of our decoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_attentional_decoder = [state_below, annotations, initial_state]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, for a sample, the sequence of annotations and initial state is the same, independently of the decoding time-step. In order to avoid computation time, we build two models, one for training and the other one for sampling. They will share weights, but the sampling model will be made up of  two different models. One (model_init) will compute the sequence of annotations and initial_stat. The other model (model_next) will compute a single recurrent step, given the sequence of annotations, the previous hidden state and the generated words up to this moment. \n",
    "\n",
    "Therefore, now we slightly change the form of declaring layers. We must share layers between the decoding models. \n",
    "\n",
    "So, let's start by building the attentional-conditional GRU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the AttGRUCond function\n",
    "sharedAttGRUCond = AttGRUCond(hidden_state_size,\n",
    "                              return_sequences=True,\n",
    "                              return_extra_variables=True, # Return attended input and attenton weights \n",
    "                              return_states=True # Returns the sequence of hidden states (see discussion above)\n",
    "                              )\n",
    "[proj_h, x_att, alphas, h_state] = sharedAttGRUCond(input_attentional_decoder) # Apply shared_AttnGRUCond to our input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we set skip connections between input and output layer. Note that, since we have a temporal dimension because of the RNN decoder, we must apply the layers in a TimeDistributed way. Finally, we will merge all skip-connections and apply a 'tanh' no-linearlity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define layer function\n",
    "shared_FC_mlp = TimeDistributed(Dense(word_embedding_size, activation='linear',),\n",
    "                                name='logit_lstm')\n",
    "# Apply layer function\n",
    "out_layer_mlp = shared_FC_mlp(proj_h)\n",
    "\n",
    "# Define layer function\n",
    "shared_FC_ctx = TimeDistributed(Dense(word_embedding_size, activation='linear'),\n",
    "                                name='logit_ctx')\n",
    "# Apply layer function\n",
    "out_layer_ctx = shared_FC_ctx(x_att)\n",
    "shared_Lambda_Permute = Lambda(lambda x: K.permute_dimensions(x, [1, 0, 2]))\n",
    "out_layer_ctx = shared_Lambda_Permute(out_layer_ctx)\n",
    "\n",
    "# Define layer function\n",
    "shared_FC_emb = TimeDistributed(Dense(word_embedding_size, activation='linear'),\n",
    "                                name='logit_emb')\n",
    "# Apply layer function\n",
    "out_layer_emb = shared_FC_emb(state_below)\n",
    "\n",
    "additional_output = merge([out_layer_mlp, out_layer_ctx, out_layer_emb], mode='sum', name='additional_input')\n",
    "shared_activation_tanh = Activation('tanh')\n",
    "out_layer = shared_activation_tanh(additional_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll' apply a deep output layer, with MaxOut activation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_maxout = TimeDistributed(MaxoutDense(word_embedding_size), name='maxout_layer')\n",
    "out_layer = shared_maxout(out_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we apply a softmax function for obtaining a probability distribution over the target vocabulary words at each timestep,."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_FC_soft = TimeDistributed(Dense(output_vocabulary_size,\n",
    "                                               activation='softmax',\n",
    "                                               name='softmax_layer'),\n",
    "                                         name=ids_outputs[0])\n",
    "softout = shared_FC_soft(out_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Include the beam search model in this tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}